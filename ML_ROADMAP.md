# Roadmap : Module d'Estimation ImmobiliÃ¨re (Machine Learning ELK)

Ce document dÃ©crit la procÃ©dure pas-Ã -pas pour transformer les donnÃ©es DVF indexÃ©es dans Elasticsearch en un moteur d'estimation de prix (RÃ©gression via Kibana ML).

## ğŸ“‹ PrÃ©requis
- [x] Stack ELK (Elasticsearch, Kibana) fonctionnelle.
- [x] DonnÃ©es DVF chargÃ©es dans un index (ex: `gov-dvf` ou `gov-dvf-paris`).
- [ ] Licence : La fonctionnalitÃ© Machine Learning nÃ©cessite une licence (Basic/Trial ou Gold+). En local/dev, l'activation de la "Trial" de 30 jours est souvent nÃ©cessaire via *Stack Management > License Management*.

---

## ğŸ“… Phase 1 : VÃ©rification des DonnÃ©es (Data Audit)
Avant d'entraÃ®ner le modÃ¨le, nous devons confirmer que les "ingrÃ©dients" sont bons.

**Action :** ExÃ©cuter dans Kibana **Dev Tools** :
```json
GET gov-dvf/_mapping
```

**Checklist des champs indispensables :**
- [ ] `valeur_fonciere` (Type: `float` ou `double`) -> **Cible (Ce qu'on veut prÃ©dire)**
- [ ] `surface_reelle_bati` (Type: `integer` ou `float`)
- [ ] `nombre_pieces_principales` (Type: `integer`)
- [ ] `latitude` (Type: `float`) 
- [ ] `longitude` (Type: `float`)
- [ ] `type_local` (Type: `keyword` ou `text`)

> ğŸ’¡ **Note :** Si `latitude` et `longitude` sont uniquement dans un objet `geo_point` (ex: `pin.location`), le ML peut les utiliser mais c'est souvent plus simple pour une rÃ©gression d'avoir les champs Ã  plat si on veut voir leur poids individuel. Cependant, Kibana gÃ¨re de mieux en mieux les Geo-types.

---

## ğŸ§  Phase 2 : CrÃ©ation du ModÃ¨le (Training)

**Outil :** Kibana > Analytics > Machine Learning > Data Frame Analytics.

### Ã‰tape 2.1 : Configuration du Job
1.  Cliquer sur **Create job**.
2.  SÃ©lectionner **Regression**.
3.  **Source index** : Choisir `gov-dvf` (ou votre index filtrÃ© `gov-dvf-paris`).
4.  **Job ID** : `estimateur_prix_immo_v1`.

### Ã‰tape 2.2 : ParamÃ¨tres d'apprentissage
5.  **Dependent variable** (La question) : SÃ©lectionner `valeur_fonciere`.
6.  **Included fields** (Les critÃ¨res) :
    *   *DÃ©cochez "All" pour Ã©viter le bruit (dates, IDs, adresses textes...)*
    *   âœ… `surface_reelle_bati`
    *   âœ… `nombre_pieces_principales`
    *   âœ… `latitude`
    *   âœ… `longitude`
    *   âœ… `type_local`
    *   *(Optionnel) `code_postal`*
7.  **Training percent** : Laisser Ã  `80` (80% entrainement, 20% test).

### Ã‰tape 2.3 : Lancement
8.  Cliquer sur **Create**.
9.  Cliquer sur **Start now**.
10. Attendre que le statut passe Ã  **Stopped** (Progression : 100%).

---

## ğŸ¯ Phase 3 : Estimation (InfÃ©rence)
Une fois le modÃ¨le entraÃ®nÃ©, il est stockÃ© dans Elasticsearch et prÃªt Ã  rÃ©pondre.

### MÃ©thode "Manuelle" (Via Dev Tools)
Pour estimer un bien spÃ©cifique ("J'ai un appart de 3 piÃ¨ces..."), utilisez la commande suivante :

```json
POST _ml/trained_models/estimateur_prix_immo_v1*/deployment/_infer
{
  "docs": [
    {
      "surface_reelle_bati": 65,      
      "nombre_pieces_principales": 3,
      "type_local": "Appartement",
      "latitude": 48.8566,
      "longitude": 2.3522
    }
  ]
}
```

### MÃ©thode "Industrielle" (IntÃ©gration)
Pour intÃ©grer cela dans votre application ou dashboard :
1.  CrÃ©er un **Ingest Pipeline** qui utilise ce processeur d'infÃ©rence.
2.  Ou appeler l'API Elasticsearch ci-dessus depuis votre code Python/API.

---

## ğŸ“Š Phase 4 : Analyse des Performances (Optionnel)
Pour vÃ©rifier si le modÃ¨le est fiable :
1.  Aller sur la liste des jobs Data Frame Analytics.
2.  Cliquer sur **View details** > **Evaluation**.
3.  Regarder le **Generalization error** (plus c'est bas, mieux c'est).
4.  Regarder l'**Importance des features** : Vous verrez probablement que la `surface` est le critÃ¨re nÂ°1, suivi de la localisation (`latitude`/`longitude`).
